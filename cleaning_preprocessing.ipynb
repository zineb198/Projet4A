{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"In this paper we consider the problem of modeling text corpora and other collections of discret. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection,       summarization, and similarity and relevance judgments.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bennis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/bennis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bennis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- clean and lower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
    "    ]\n",
    "    for rule in rules:\n",
    "        for (k, v) in rule.items():\n",
    "            regex = re.compile(k)\n",
    "            text = regex.sub(v, text)\n",
    "        text = text.rstrip()\n",
    "    return text.lower()\n",
    "\n",
    "text_clean = text_cleaner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'this', 'paper', 'we', 'consider', 'the', 'problem', 'of', 'modeling', 'text', 'corpora', 'and', 'other', 'collections', 'of', 'discret.', 'the', 'goal', 'is', 'to', 'find', 'short', 'descriptions', 'of', 'the', 'members', 'of', 'a', 'collection', 'that', 'enable', 'efficient', 'processing', 'of', 'large', 'collections', 'while', 'preserving', 'the', 'essential', 'statistical', 'relationships', 'that', 'are', 'useful', 'for', 'basic', 'tasks', 'such', 'as', 'classification,', 'novelty', 'detection,', 'summarization,', 'and', 'similarity', 'and', 'relevance', 'judgments.']\n"
     ]
    }
   ],
   "source": [
    "text_split = text_clean.split()\n",
    "print(text_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'this', 'paper', 'we', 'consider', 'the', 'problem', 'of', 'modeling', 'text', 'corpora', 'and', 'other', 'collections', 'of', 'discret', 'the', 'goal', 'is', 'to', 'find', 'short', 'descriptions', 'of', 'the', 'members', 'of', 'a', 'collection', 'that', 'enable', 'efficient', 'processing', 'of', 'large', 'collections', 'while', 'preserving', 'the', 'essential', 'statistical', 'relationships', 'that', 'are', 'useful', 'for', 'basic', 'tasks', 'such', 'as', 'classification', 'novelty', 'detection', 'summarization', 'and', 'similarity', 'and', 'relevance', 'judgments']\n"
     ]
    }
   ],
   "source": [
    "punctuation = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(punctuation) for w in text_split]\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'consider', 'problem', 'modeling', 'text', 'corpora', 'collections', 'discret', 'goal', 'find', 'short', 'descriptions', 'members', 'collection', 'enable', 'efficient', 'processing', 'large', 'collections', 'preserving', 'essential', 'statistical', 'relationships', 'useful', 'basic', 'tasks', 'classification', 'novelty', 'detection', 'summarization', 'similarity', 'relevance', 'judgments']\n"
     ]
    }
   ],
   "source": [
    "#enlève les mots \"inutiles\": pronoms, déterminants, prépositions\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered = [w for w in stripped if not w in stop_words]\n",
    "\n",
    "filtered = []\n",
    "\n",
    "for w in stripped:\n",
    "    if w not in stop_words:\n",
    "        filtered.append(w)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-101-f722f375184e>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-101-f722f375184e>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    pip install autocorrect\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#spell correction marche pas\n",
    "from nltk.metrics import edit_distance\n",
    "from autocorrect import spell\n",
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this paper we consider the problem of modeling text corpora and other collections of discretdthe goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n"
     ]
    }
   ],
   "source": [
    "#abbrevations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n",
      "consid\n",
      "problem\n",
      "model\n",
      "text\n",
      "corpora\n",
      "collect\n",
      "discret\n",
      "goal\n",
      "find\n",
      "short\n",
      "descript\n",
      "member\n",
      "collect\n",
      "enabl\n",
      "effici\n",
      "process\n",
      "larg\n",
      "collect\n",
      "preserv\n",
      "essenti\n",
      "statist\n",
      "relationship\n",
      "use\n",
      "basic\n",
      "task\n",
      "classif\n",
      "novelti\n",
      "detect\n",
      "summar\n",
      "similar\n",
      "relev\n",
      "judgment\n"
     ]
    }
   ],
   "source": [
    "#stemming et lemmatizer font des choses differents; a voir le plus efficace\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = filtered\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n",
      "consider\n",
      "problem\n",
      "modeling\n",
      "text\n",
      "corpus\n",
      "collection\n",
      "discret\n",
      "goal\n",
      "find\n",
      "short\n",
      "description\n",
      "member\n",
      "collection\n",
      "enable\n",
      "efficient\n",
      "processing\n",
      "large\n",
      "collection\n",
      "preserving\n",
      "essential\n",
      "statistical\n",
      "relationship\n",
      "useful\n",
      "basic\n",
      "task\n",
      "classification\n",
      "novelty\n",
      "detection\n",
      "summarization\n",
      "similarity\n",
      "relevance\n",
      "judgment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bennis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "example_words = filtered\n",
    "\n",
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
