{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"In this paper we consider the problem of modeling text corpora and other collections of discretdThe goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection,       summarization, and similarity and relevance judgments.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this paper we consider the problem of modeling text corpora and other collections of discretdthe goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n",
      "['in', 'this', 'paper', 'we', 'consider', 'the', 'problem', 'of', 'modeling', 'text', 'corpora', 'and', 'other', 'collections', 'of', 'discretdthe', 'goal', 'is', 'to', 'find', 'short', 'descriptions', 'of', 'the', 'members', 'of', 'a', 'collection', 'that', 'enable', 'efficient', 'processing', 'of', 'large', 'collections', 'while', 'preserving', 'the', 'essential', 'statistical', 'relationships', 'that', 'are', 'useful', 'for', 'basic', 'tasks', 'such', 'as', 'classification', ',', 'novelty', 'detection', ',', 'summarization', ',', 'and', 'similarity', 'and', 'relevance', 'judgments', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bennis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#minuscules\n",
    "text =text.lower()\n",
    "#noise removal, voir si on peut enlever la ponctuation\n",
    "import re\n",
    "def text_cleaner(text):\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
    "    ]\n",
    "    for rule in rules:\n",
    "        for (k, v) in rule.items():\n",
    "            regex = re.compile(k)\n",
    "            text = regex.sub(v, text)\n",
    "        text = text.rstrip()\n",
    "    return text.lower()\n",
    "print(text_cleaner(text))\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-f722f375184e>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-f722f375184e>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    pip install autocorrect\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#spell correction marche pas\n",
    "from nltk.metrics import edit_distance\n",
    "from autocorrect import spell\n",
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'this', 'paper', 'we', 'consider', 'the', 'problem', 'of', 'modeling', 'text', 'corpora', 'and', 'other', 'collections', 'of', 'discretdthe', 'goal', 'is', 'to', 'find', 'short', 'descriptions', 'of', 'the', 'members', 'of', 'a', 'collection', 'that', 'enable', 'efficient', 'processing', 'of', 'large', 'collections', 'while', 'preserving', 'the', 'essential', 'statistical', 'relationships', 'that', 'are', 'useful', 'for', 'basic', 'tasks', 'such', 'as', 'classification', ',', 'novelty', 'detection', ',', 'summarization', ',', 'and', 'similarity', 'and', 'relevance', 'judgments', '.']\n",
      "['paper', 'consider', 'problem', 'modeling', 'text', 'corpora', 'collections', 'discretdthe', 'goal', 'find', 'short', 'descriptions', 'members', 'collection', 'enable', 'efficient', 'processing', 'large', 'collections', 'preserving', 'essential', 'statistical', 'relationships', 'useful', 'basic', 'tasks', 'classification', ',', 'novelty', 'detection', ',', 'summarization', ',', 'similarity', 'relevance', 'judgments', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bennis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#enlève les mots \"inutiles\": pronoms, déterminants, prépositions\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = tokens\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "phrase=filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this paper we consider the problem of modeling text corpora and other collections of discretdthe goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n"
     ]
    }
   ],
   "source": [
    "#abbrevations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n",
      "consid\n",
      "problem\n",
      "model\n",
      "text\n",
      "corpora\n",
      "collect\n",
      "discretdth\n",
      "goal\n",
      "find\n",
      "short\n",
      "descript\n",
      "member\n",
      "collect\n",
      "enabl\n",
      "effici\n",
      "process\n",
      "larg\n",
      "collect\n",
      "preserv\n",
      "essenti\n",
      "statist\n",
      "relationship\n",
      "use\n",
      "basic\n",
      "task\n",
      "classif\n",
      ",\n",
      "novelti\n",
      "detect\n",
      ",\n",
      "summar\n",
      ",\n",
      "similar\n",
      "relev\n",
      "judgment\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#stemming a voir\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = filtered_sentence\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bennis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "example_words = filtered_sentence\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "print(lemmatizer.lemmatize(\"animals\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
