{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some useful functions for computing the *tf-idf* for the data with arXiv abstracts.\n",
    "\n",
    "Note here the use of specific pandas function like *concat*, *value_counts* and *groupby* which make possible to speed up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./arxiv_articles.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_primary_category</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://arxiv.org/abs/2001.05867v1</td>\n",
       "      <td>$σ$-Lacunary actions of Polish groups</td>\n",
       "      <td>Jan Grebik</td>\n",
       "      <td>math.LO</td>\n",
       "      <td>We show that every essentially countable orbit...</td>\n",
       "      <td>2020-01-16T15:09:02Z</td>\n",
       "      <td>2020-01-16T15:09:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://arxiv.org/abs/1303.6933v1</td>\n",
       "      <td>Hans Grauert (1930-2011)</td>\n",
       "      <td>Alan Huckleberry</td>\n",
       "      <td>math.HO</td>\n",
       "      <td>Hans Grauert died in September of 2011. This a...</td>\n",
       "      <td>2013-03-27T19:23:57Z</td>\n",
       "      <td>2013-03-27T19:23:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://arxiv.org/abs/1407.3775v1</td>\n",
       "      <td>A New Proof of Stirling's Formula</td>\n",
       "      <td>Thorsten Neuschel</td>\n",
       "      <td>math.HO</td>\n",
       "      <td>A new simple proof of Stirling's formula via t...</td>\n",
       "      <td>2014-07-10T11:26:39Z</td>\n",
       "      <td>2014-07-10T11:26:39Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://arxiv.org/abs/math/0307381v3</td>\n",
       "      <td>On Dequantization of Fedosov's Deformation Qua...</td>\n",
       "      <td>Alexander V. Karabegov</td>\n",
       "      <td>math.QA</td>\n",
       "      <td>To each natural deformation quantization on a ...</td>\n",
       "      <td>2003-07-30T06:20:33Z</td>\n",
       "      <td>2003-09-20T01:29:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://arxiv.org/abs/1604.06794v1</td>\n",
       "      <td>Cyclic extensions are radical</td>\n",
       "      <td>Mariano Suárez-Álvarez</td>\n",
       "      <td>math.HO</td>\n",
       "      <td>We show that finite Galois extensions with cyc...</td>\n",
       "      <td>2016-04-21T22:24:54Z</td>\n",
       "      <td>2016-04-21T22:24:54Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>http://arxiv.org/abs/1712.09576v2</td>\n",
       "      <td>The Second Main Theorem in the hyperbolic case</td>\n",
       "      <td>Min Ru;Nessim Sibony</td>\n",
       "      <td>math.CV</td>\n",
       "      <td>We develop Nevanlinna's theory for a class of ...</td>\n",
       "      <td>2017-12-27T13:17:08Z</td>\n",
       "      <td>2019-01-03T07:51:11Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0    http://arxiv.org/abs/2001.05867v1   \n",
       "1     http://arxiv.org/abs/1303.6933v1   \n",
       "2     http://arxiv.org/abs/1407.3775v1   \n",
       "3  http://arxiv.org/abs/math/0307381v3   \n",
       "4    http://arxiv.org/abs/1604.06794v1   \n",
       "5    http://arxiv.org/abs/1712.09576v2   \n",
       "\n",
       "                                               title                 authors  \\\n",
       "0              $σ$-Lacunary actions of Polish groups              Jan Grebik   \n",
       "1                           Hans Grauert (1930-2011)        Alan Huckleberry   \n",
       "2                  A New Proof of Stirling's Formula       Thorsten Neuschel   \n",
       "3  On Dequantization of Fedosov's Deformation Qua...  Alexander V. Karabegov   \n",
       "4                      Cyclic extensions are radical  Mariano Suárez-Álvarez   \n",
       "5     The Second Main Theorem in the hyperbolic case    Min Ru;Nessim Sibony   \n",
       "\n",
       "  arxiv_primary_category                                            summary  \\\n",
       "0                math.LO  We show that every essentially countable orbit...   \n",
       "1                math.HO  Hans Grauert died in September of 2011. This a...   \n",
       "2                math.HO  A new simple proof of Stirling's formula via t...   \n",
       "3                math.QA  To each natural deformation quantization on a ...   \n",
       "4                math.HO  We show that finite Galois extensions with cyc...   \n",
       "5                math.CV  We develop Nevanlinna's theory for a class of ...   \n",
       "\n",
       "              published               updated  \n",
       "0  2020-01-16T15:09:02Z  2020-01-16T15:09:02Z  \n",
       "1  2013-03-27T19:23:57Z  2013-03-27T19:23:57Z  \n",
       "2  2014-07-10T11:26:39Z  2014-07-10T11:26:39Z  \n",
       "3  2003-07-30T06:20:33Z  2003-09-20T01:29:18Z  \n",
       "4  2016-04-21T22:24:54Z  2016-04-21T22:24:54Z  \n",
       "5  2017-12-27T13:17:08Z  2019-01-03T07:51:11Z  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naif_regex_tokenize(text):\n",
    "    \"\"\"\n",
    "    This is a very naif way of tokenize a text. Just using the\n",
    "    regular expression \"[a-z]\" that will match any single word\n",
    "    in lowercase.\n",
    "    Returns a list with all the tokens.\n",
    "    \"\"\"\n",
    "    p = re.compile(\"[a-z]+\")\n",
    "    return p.findall(text.lower())\n",
    "\n",
    "def compute_tf(d):\n",
    "    \"\"\"\n",
    "    Compute the tf for a given document d.\n",
    "    The formula used is \n",
    "    \n",
    "        tf(t, d) = 0.5 + 0.5 * (count(t, d)/max(count(t',d) for t' in d))\n",
    "    \n",
    "    This prevents bias in longer documents.\n",
    "    \"\"\"\n",
    "    terms = pd.Series(naif_regex_tokenize(d))\n",
    "    term_counts = terms.value_counts()\n",
    "    max_tc = max(term_counts)\n",
    "    return 0.5 + 0.5 * (term_counts / max_tc)\n",
    "\n",
    "def compute_idf(D):\n",
    "    \"\"\"\n",
    "    The input D is a list of pandas.Series\n",
    "    having as each element, the term frequency \n",
    "    computed by the function compute_tf.\n",
    "    \"\"\"\n",
    "    N = len(D)\n",
    "    all_terms = pd.concat(D)\n",
    "    nt = all_terms.index.value_counts() # The number of documents containing the term \"t\"\n",
    "    return np.log(N /(1+  nt))\n",
    "\n",
    "def compute_tf_idf_document(tf_document, idf):\n",
    "    \"\"\"Compute the tf-idf for each term in a document of the corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    tf_document -- list with the frequency of each term inside the document\n",
    "    idf -- the idf value for each term in the corpus\n",
    "    \"\"\"\n",
    "    return tf_document * np.array([idf[i] for i in tf_document.index])\n",
    "    \n",
    "def compute_tf_idf_corpus(D):\n",
    "    \"\"\"Compute the tf-idf for each term in a corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    D -- pandas Series containing a collection of documents in text format\n",
    "    \n",
    "    returns\n",
    "        list of pandas Series containing the tf-idf(t, d, D) for each term\n",
    "        inside each document of the corpus D\n",
    "    \"\"\"\n",
    "    term_freq = [compute_tf(d) for d in D]\n",
    "    idf = compute_idf(term_freq)\n",
    "    return [compute_tf_idf_document(d, idf) for d in term_freq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = compute_tf_idf_corpus(data.loc[:, \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a                  0.101143\n",
       " essentially        4.210235\n",
       " that               0.358230\n",
       " polish             6.765908\n",
       " is                 0.241595\n",
       " of                 0.021311\n",
       " we                 0.210556\n",
       " action             3.321325\n",
       " equivalence        3.869255\n",
       " group              2.693323\n",
       " countable          5.583588\n",
       " by                 0.483113\n",
       " induced            2.871189\n",
       " every              3.201351\n",
       " math               4.575787\n",
       " relation           2.930591\n",
       " show               0.901848\n",
       " space              1.601042\n",
       " continuous         2.002911\n",
       " lacunary           5.952641\n",
       " orbit              2.951838\n",
       " obtain             1.943206\n",
       " adv                5.339623\n",
       " abelian            4.318291\n",
       " in                 0.076026\n",
       " archimedean        4.766941\n",
       " hyperfinite        6.206057\n",
       " on                 0.349148\n",
       " straightforward    3.128792\n",
       " combination        2.385697\n",
       " invent             5.339623\n",
       " with               0.318536\n",
       " proof              2.716372\n",
       " result             1.686051\n",
       " non                1.320419\n",
       " the                0.011738\n",
       " an                 0.474913\n",
       " from               0.582492\n",
       " sigma              2.918497\n",
       " dtype: float64, in                 0.121641\n",
       " his                4.656692\n",
       " mathematics        3.932206\n",
       " died               5.842219\n",
       " this               0.350482\n",
       " article            2.477230\n",
       " recalls            6.240190\n",
       " some               1.532707\n",
       " and                0.061426\n",
       " grauert            7.447268\n",
       " detail             3.146050\n",
       " major              2.689358\n",
       " accomplishments    6.407548\n",
       " hans               7.143170\n",
       " life               2.902926\n",
       " of                 0.018267\n",
       " reviews            4.229424\n",
       " september          4.719079\n",
       " dtype: float64, the          0.018781\n",
       " formula      3.116355\n",
       " stirling     6.760050\n",
       " new          1.300458\n",
       " for          0.235589\n",
       " s            1.354934\n",
       " presented    2.171808\n",
       " is           0.207081\n",
       " partial      2.851298\n",
       " via          2.229145\n",
       " function     1.639328\n",
       " expansion    3.221176\n",
       " tangent      5.300618\n",
       " simple       1.880937\n",
       " fraction     3.064610\n",
       " a            0.075857\n",
       " of           0.018267\n",
       " proof        3.259646\n",
       " dtype: float64]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have the tf-idf values at each document. In order to select the *most important* terms (i.e. the terms with higher tf-idf values), we compute the **mean** of the tf-idf for each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms = pd.concat(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_terms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44174a8cf7bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_terms' is not defined"
     ]
    }
   ],
   "source": [
    "mean_tf_idf = all_terms.groupby(all_terms.index).mean()\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tf_idf = mean_tf_idf.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karatsuba             10.622838\n",
       "erdi                  10.622838\n",
       "sagetex               10.622838\n",
       "bioelectrodynamics    10.622838\n",
       "koras                 10.622838\n",
       "                        ...    \n",
       "verschiebung           7.967129\n",
       "verses                 7.967129\n",
       "carlisle               7.967129\n",
       "cotter                 7.967129\n",
       "cotranslational        7.967129\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tf_idf[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.967128798944532"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms[\"cotter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique[\"cotter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant c'est à vous\n",
    "\n",
    "Utiliser le code et les fonctions ci-dessus pour :\n",
    "\n",
    "1. Proposer un dictionnaire des termes (une liste de mots qui peuvent être \"informatives\"). Par exemple, un mot qui est présent dans plus de 10 documents différents et qui a un fort valeur de *tf-idf* peut etre \"informatif\". \n",
    "\n",
    "2. Utiliser ces termes pour calculer, pour chaque document, un vecteur *tf-idf*. Ce vecteur aura les valeurs du tf-idf de chaqu'un des termes. Par example, si la liste de termes est [\"float\", \"genetic\", \"circular\"] et qu'on a 4 document. On doit produire une matrice de 4 lignes et 3 colonnes :\n",
    "```\n",
    "0.1 5.8 9\n",
    "4.7 1.0 3\n",
    "8.0 2.4 6.0\n",
    "0.3 9.1 3.2\n",
    "```   \n",
    "Ici, chaque ligne contient les valeurs de *tf-idf* de [\"float\", \"genetic\", \"circular\"] (dans cet ordre).\n",
    "\n",
    "3. Normaliser les lignes de cette matrice. La norme 2 des vecteurs *tf-idf* représentés à chaque ligne doit être 1. Les étapes **2** et **3** font partie de ce qu'on appelle *feature extraction*. \n",
    "\n",
    "4. Executer l'exemple décrit [ici](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Appliquer la même analyse sur notre jeu des données.\n",
    "\n",
    "5. Faire une implémentation de l'algorithme k-means. Voir les liens : https://en.wikipedia.org/wiki/K-means_clustering, https://en.wikipedia.org/wiki/K-means%2B%2B, https://fr.wikipedia.org/wiki/K-moyennes\n",
    "\n",
    "6. Implémenter une fonction permettant de trier les documents en fonction du résultat de l'algorithme k-means avec **k** groupes.\n",
    "\n",
    "7. Executer l'exemple décrit [ici](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py) puis appliquer la même analyse sur notre jeu des données\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
